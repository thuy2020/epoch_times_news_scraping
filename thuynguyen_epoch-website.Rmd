---
title: "EDLD 609: Scrcaping The Epoch Times website"
author: "Thuy Nguyen"
date: "1/20/2021"
output: html_document
---
## Project's goals
Goal 1: Scraping articles from the website https://www.theepochtimes.com/c-us-politics, extracting needed information, and putting them into an appropriate format to conduct text analysis. This is a part of a bigger research project in collaboration with the US-Vietnam Research Center, UO. 
Goal 2: Writing a package to scrape articles from a news website.

## Progress made: 
- Getting one article from a link
- Getting a set of links from 1 page (out of total 387 pages needed)
- Extracting needed information from 1 article to be used for text analysis

## To do tasks: 

- Get all links from 387 pages in politics section: https://www.theepochtimes.com/c-us-politics
- Get all articles from these 387 pages
- Extracting needed information from these articles
- Converting extracted information into an appropriate format for text analysis 

## Would like to discuss: 

- The feasibility/ necessity of goal 2 
- the accuracy of the work I have done below. 
- Other tasks to fulfill EDLD 609's requirements. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(rvest)
library(tidyverse)
library(xml2)
library(httr) 
library(tidytext)
```

# Function to get one article from a link 

```{r get_article}
# a function to get an article (adapted from ICPSR 2020 course, link: )
get_article <- function(url, save = TRUE, path = NULL){
  page = httr::GET(url)
  page_content = httr::content(page, "text")
  if(save == TRUE){
    write.table(x = page_content,
                col.names = FALSE,
                row.names = FALSE,
                quote = FALSE,
                file = path
                )
  }
  return(page_content)
}

# get one article from a link
article <- get_article(url = "https://www.theepochtimes.com/trump-grants-full-pardon-to-al-pirro-former-husband-of-fox-news-host-jeanine-pirro_3664393.html", save = TRUE, path = "data.html") # how to save all articles in a folder?

```

# Function to extract features of interest from one article

```{r get_features}
get_features <- function(article) {
# read in as html file 
article_html <- read_html(article)

# get title
title <- article_html %>% 
  xml_find_all("//title") %>% 
  html_text()

# get autho's name <p><em> --> Get node <em> then extract the text
autho_name <- html_text(xml_find_all(article_html, "//em"))

# get post content class using . <div class="post_content">
post_content <- article_html %>% 
  html_nodes(css = ".post_content") %>% 
  html_text

# get date publish & update <span class="publish"> 
publish_date <- article_html %>% 
  html_nodes(css = ".publish") %>% 
  html_text()

publish_date_update <- article_html %>% 
  html_nodes(css = ".update") %>% 
  html_text()

# get autho's name <p><em> --> Get node <em> then extract the text
autho_name <- html_text(xml_find_all(article_html, "//em"))

# get description <meta name="description"
# select all meta nodes in the tree, then go to their children that have name = description. 
description <- article_html %>% 
  html_nodes(xpath = '//*[@id="main"]/div/div/div[1]/div[1]/div[2]/div[5]/p[1]') %>% 
  html_text()

tbl <- tibble(
       title = title, 
       author = autho_name, 
       publish_date = lubridate::mdy(publish_date),
       publish_update = lubridate::mdy(gsub("Updated: ", "", publish_date_update)),
       description = description,
       content = post_content)

return (tbl)
}

get_features(article)

```

# Function to get all article links from one page

```{r get_article_links}

# need to get all the links of post_list in a page, then get html of each link 
# <li class="post_list">  --> <a title= "mcbvksd" href="a link inside"
get_article_links <- function(url) {
  
one_page <- get_article(url = url,
                        save = TRUE, 
                        path = "first_page.html")
# convert to html file to extract elements in it. 
one_page <- read_html(one_page)

# get all the articles' links in this page
# go to all element that have class = "post_list", then go to the "a" descendant in the first position. ---> get 27 X 3 = 81 lines, 3 for each article 
raw_list <- one_page %>% 
  html_nodes(xpath = "//*[@class = 'post_list']//a[position() = 1]") 

# extract the link, starting the second in the list, increment by 3, 
urls <- raw_list[seq(2, length(raw_list), 3)] # 27 links (articles) in a page

# convert from html to character string in order to use stringr
urls_char <- as.character(urls)

# split the string to take only the link part
urls_char <- str_split(
  urls_char,
  pattern = ">",
  simplify = TRUE # to get a matrix (instead of a list), each column is an element being splited, here 27 x 3, the first column is the link we want.
)

# get rid of all the first part <a href=\" before the link
urls_char <- str_replace(urls_char[ ,1], # only get the first column which contains links
           pattern = '<a href=\"', 
           replacement = "")

# get rid of all the \" part at the end of the link
urls_clean <- str_replace(urls_char,
           pattern = '\"', 
           replacement = "")

return(urls_clean)
}

get_article_links(url = "https://www.theepochtimes.com/c-us-politics/3")
```

# Get all pages

```{r get all the urls of articles in 1 page }
get_page_link <- function(page) {
  link <- "https://www.theepochtimes.com/c-us-politics"
  if(page > 1) {
    link <- paste0(link, "/", page)
  }
  
  link
}

get_page_link(3)

all_page_links <- map_chr(1:677, get_page_link)

map(all_page_links[1:5], get_links)

```

```{r extract needed infrom from a page }
# Define a delayed read_html() function with a delay of 1s
read_html_delayed <- slowly(read_html, 
                            rate = rate_delay(1))

# A loop that goes over all urls in a list of urls

for(url in urls_clean){
 # html <- read_html_delayed(url) can't run 
 article <- web_scraping(url, save = TRUE, path = NULL)
 #features <- extract_features(article) 
 return (article) 
}

read_html_delayed(urls_clean)

```

```{r analysis}

tbl %>% 
  unnest_tokens(word, content) %>% 
  anti_join(stop_words) %>% 
  count(word, sort = TRUE)
  View()
```


```{r unresolved}
# get keywords <meta name="keywords" using a xpath--> how to get up to "keywords"???
keyword <- xml_find_all(article_html, xpath = "//meta/@name/@content")
html_nodes(css = "#content")

# get share count <div id="share_counts" class="share_counts">
share_count <- html_nodes(article_html, css = "#share_counts")
html_text(share_count) #???
```
