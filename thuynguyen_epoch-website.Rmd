---
title: "EDLD 609: Scrcaping The Epoch Times website"
author: "Thuy Nguyen"
date: "1/20/2021"
output: html_document
---
## Project's goals
Goal 1: Scraping articles from the website https://www.theepochtimes.com/c-us-politics, extracting needed information, and putting them into an appropriate format to conduct text analysis. This is a part of a bigger research project in collaboration with the US-Vietnam Research Center, UO. 
Goal 2: Conducting a preliminary analysis of the data

## Progress made since last report:

created 4 visualizations on a sample groups of articles: 
- Words occur more than 3 times in each document
- word cloud
- Words associated with sentiments using "nrc" dictionary
- sentiments using "bing"

## To do tasks: 

- Extract all articles features of interest from the links in all_article_links.RDS.
- Create visualizations on a larger sample set
- Writing report

## Would like to discuss:

- Format of the final product for submission. Probably an interactive report?
 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r lib}
library(rvest)
library(tidyverse)
library(xml2)
library(httr) 
library(tidytext)
library(stringr)
```

# Function to get one article from a url link 

```{r get_article}
# a function to get an article 
get_article <- function(url){
  link <- GET(url)
  content <- httr::content(link, "text")
  
  return(content)
}

# apply function to get one article from a link
article <- get_article(url = "https://www.theepochtimes.com/texas-governor-orders-agencies-to-sue-biden-administration-for-climate-actions-that-kill-jobs_3677062.html") 
```

# Function to extract features of interest from one article

```{r get_features}
get_features <- function(article) {
# read in as html file 
article_html <- read_html(article)

# get title
title <- article_html %>% 
  xml_find_all("//title") %>% 
  html_text()

# get post content class using . <div class="post_content">
post_content <- article_html %>% 
  html_nodes(css = ".post_content") %>% 
  html_text %>% 
  str_replace("\n\t\t\t\t\t\t\t", "")

# get date publish & update <span class="publish"> 
publish_date <- article_html %>% 
  html_nodes(css = ".publish") %>% 
  html_text() 

publish_update <- article_html %>% 
  html_nodes(css = ".update") %>% 
  html_text() 

# get autho's name <p><em> --> Get node <em> then extract the text
autho_name <- html_text(xml_find_all(article_html, "//em"))

# get description <meta name="description"
# select all meta nodes in the tree, then go to their children that have name = description. 
description <- article_html %>% 
  html_nodes(xpath = '//*[@id="main"]/div/div/div[1]/div[1]/div[2]/div[5]/p[1]') %>% 
  html_text()

tbl <- tibble(
         title = ifelse(is.null(title), "", title),
         author = ifelse(is.null(autho_name), "", autho_name), 
         publish_date = ifelse(is.null(publish_date), "", publish_date),
         publish_update = ifelse(is.null(publish_update), "", publish_update),
         description = ifelse(is.null(description), "", description),
         content = ifelse(is.null(post_content), "", post_content)
)
tbl
}

# test the function on 1 article 
get_features(article)

```

# Function to get all 27 links from one page

```{r get_article_links}
# need to get all the links of post_list in a page, then get html of each link 
# <li class="post_list">  --> <a title= "mcbvksd" href="a link inside"
get_article_links <- function(url) {
  
one_page <- get_article(url = url)
# convert to html file to extract elements in it. 
one_page <- read_html(one_page)

# get all the articles' links in this page
# go to all element that have class = "post_list", then go to the "a" descendant in the first position. 
raw_list <- one_page %>% 
  html_nodes(xpath = "//*[@class = 'post_list']//a[position() = 1]") 

# extract the link, starting the second in the list, increment by 3, 
urls <- raw_list[seq(2, length(raw_list), 3)] # 27 links (articles) in a page

# convert from html to character string in order to use stringr
urls_char <- as.character(urls)

# split the string to get only the link part
urls_char <- str_split(
  urls_char,
  pattern = ">",
  simplify = TRUE # to get a matrix (instead of a list), each column is an element being splited, here 27 x 3, the first column is the link we want.
)

# get rid of all the first part <a href=\" before the link
urls_char <- str_replace(urls_char[ ,1], # only get the first column which contains links
           pattern = '<a href=\"', 
           replacement = "")

# get rid of all the \" part at the end of the link
urls_clean <- str_replace(urls_char,
           pattern = '\"', 
           replacement = "")

return(urls_clean)
}

# apply function to get links in one page
links_in_one_page <- get_article_links(url = "https://www.theepochtimes.com/c-us-politics/10")
```

# A for loop to get get all 27 articles from 27 links in one page

```{r for loop getting all 27 articles in 1 page}
# create an empty list to store the result in
test_list <- list()
for (url in 1:length(links_in_one_page)) {
# delay function get_article to run slowly
 get_article_delayed <- slowly(get_article, 
                             rate = rate_delay(1))
  
articles <- get_article_delayed(url = links_in_one_page[[url]])
  for (i in articles) {
    art_features  <- get_features(i)
        test_list[[i]] <- art_features
  }
  
}

length(test_list)
```

# Get all urls of all articles in politics section

```{r eval=FALSE, get_page_link function, all_article_links, }
# A funtion to get page links
get_page_link <- function(page) {
  link <- "https://www.theepochtimes.com/c-us-politics"
  if(page > 1) {
    link <- paste0(link, "/", page)
  }
    link
}
# Get page links, map through a vector of all pages 2:691
all_page_links <- map_chr(2:691, get_page_link) 

# get all article links from above page links, 
all_article_links <- map(all_page_links, slowly(get_article_links,
                                                     rate = rate_delay(0.2))  
                         ) %>% 
                    unlist() 

length(all_article_links) # hoorray 18657 articles to read

# save and restore 
saveRDS(all_article_links, file = "all_article_links")
```

# cleaning links

```{r cleaning a test set of links}
all_article_links <- readRDS(file = "all_article_links")

# test on a small set of links. random checking reveals that some links are not clean. eg: 
testlinks <- all_article_links[20:30] 
test_wrong_links <- testlinks[str_detect(testlinks, pattern = "<a") == TRUE] 
test_correct_links <- testlinks[str_detect(testlinks, pattern = "<a") == FALSE] 

test_wrong_links_split <- test_wrong_links %>% 
  str_split(pattern = 'href=\"', simplify = TRUE) # split the string to take only the link part

# subset to keep the column with the correct part 
test_fixed_wrong_links <- test_wrong_links_split[, 2] %>% 
                     str_replace(pattern = '\"', replacement = "") # get rid of the "\" part at the end 
# detect strings that has "<a", subset to only keep those wrong links. i.e those with the defined pattern = TRUE

# listing the fixed links back 
test_total <- unlist(c(test_correct_links, test_fixed_wrong_links)) 
```

```{r cleaning all_article_links, save as total_links}

wrong_links <- all_article_links[str_detect(all_article_links, pattern = "<a") == TRUE] 
correct_links <- all_article_links[str_detect(all_article_links, pattern = "<a") == FALSE] 

wrong_links <- wrong_links %>% 
  str_split(pattern = 'href=\"', simplify = TRUE) # split the string to take only the link part

# subset to keep the column with the correct part 
wrong_links_fixed <- wrong_links[, 2] %>% 
                     str_replace(pattern = '\"', replacement = "") # get rid of the "\" part at the end 

total_links <- unlist(c(correct_links, wrong_links_fixed)) # make them a list, then unlist to get correct length --> must be some better way to do this? 
```
# Extracting features 

## Extracting features on a test set of links

```{r extract features from a test set}
test5 <- total_links[1:5]
# create an empty list to store the result

get_article_features <- function(link) {
  get_article(link) %>% 
    get_features()
}

get_article_features(test5[1])

test5_articles <- map_df(test5, ~{
    Sys.sleep(0.5)
    get_article_features(.x)
  },
  .id = "internal_article_id")

View(test5_articles)
```


```{r get result}
# AM I READY TO GO?
result <- list()

for (url in 1:length(total_links)) {
# delay function get_article to run slowly 0.5s
 get_article_delayed <- slowly(get_article, 
                             rate = rate_delay(0.5))
  
  articles <- get_article_delayed(url = total_links[[url]], save = TRUE, path = "test.html")
    
  for (i in articles) {
    # delay function get_features to run slowly 1s
    get_features_delayed <- slowly(get_features, 
                                   rate = rate_delay(1))
    
    art_features  <- get_features_delayed(i)
  }
  
   result[[i]] <- art_features
}
```


