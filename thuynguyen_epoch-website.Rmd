---
title: "EDLD 609: Scrcaping The Epoch Times website"
author: "Thuy Nguyen"
date: "1/20/2021"
output: html_document
---
## Project's goals
Goal 1: Scraping articles from the website https://www.theepochtimes.com/c-us-politics, extracting needed information, and putting them into an appropriate format to conduct text analysis. This is a part of a bigger research project in collaboration with the US-Vietnam Research Center, UO. 
Goal 2: Writing a package to scrape articles from a news website.

## Progress made: 
- Getting one article from a link
- Getting a set of links from 1 page (out of total 387 pages needed)
- Extracting needed information from 1 article to be used for text analysis

## To do tasks: 

- Get all links from 387 pages in politics section: https://www.theepochtimes.com/c-us-politics
- Get all articles from these 387 pages
- Extracting needed information from these articles
- Converting extracted information into an appropriate format for text analysis 

## Would like to discuss: 

- The feasibility/ necessity of goal 2 
- the accuracy of the work I have done below. 
- Other tasks to fulfill EDLD 609's requirements. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(rvest)
library(tidyverse)
library(xml2)
library(httr) 
library(tidytext)
library(stringr)
```

# Function to get one article from a link 

```{r get_article}
# a function to get an article (adapted from ICPSR 2020 course, link: )
get_article <- function(url, save = TRUE, path = NULL){
  link <- GET(url)
  content <- httr::content(link, "text")
  if(save == TRUE){
    write.table(x = content,
                col.names = FALSE,
                row.names = FALSE,
                quote = FALSE,
                file = path
                )
  }
  return(content)
}

# get one article from a link
article5 <- get_article(url = "https://www.theepochtimes.com/texas-governor-orders-agencies-to-sue-biden-administration-for-climate-actions-that-kill-jobs_3677062.html", save = TRUE, path = "data5.html") 

# how to save all articles in a folder?
# create path to all articles, loops through those path. 
```

# Function to extract features of interest from one article

```{r get_features}
get_features <- function(article) {
# read in as html file 
article_html <- read_html(article)

# get title
title <- article_html %>% 
  xml_find_all("//title") %>% 
  html_text()

# get post content class using . <div class="post_content">
post_content <- article_html %>% 
  html_nodes(css = ".post_content") %>% 
  html_text

# get date publish & update <span class="publish"> 
publish_date <- article_html %>% 
  html_nodes(css = ".publish") %>% 
  html_text()

publish_date_update <- article_html %>% 
  html_nodes(css = ".update") %>% 
  html_text()

# get autho's name <p><em> --> Get node <em> then extract the text
autho_name <- html_text(xml_find_all(article_html, "//em"))

# get description <meta name="description"
# select all meta nodes in the tree, then go to their children that have name = description. 
description <- article_html %>% 
  html_nodes(xpath = '//*[@id="main"]/div/div/div[1]/div[1]/div[2]/div[5]/p[1]') %>% 
  html_text()

# tbl <- tibble(
#        title = title, 
#        author = autho_name, 
#        publish_date = lubridate::mdy(publish_date),
#        publish_update = lubridate::mdy(gsub("Updated: ", "", publish_date_update)),
#        description = description,
#        content = post_content)
# ---> the above will not give result of one feature is missing

mydata <- cbind(
       title, 
       autho_name, 
       publish_date,
       publish_date_update,
       description, post_content)

return (mydata)
}

get_features(article)

```


```{r testing}
# this link doesn't not give result because author name was not extracted --> need to use cbind instead of tiblle to gather all feature for final result
article1 <- get_article(url = "https://www.theepochtimes.com/democrats-introduce-bill-to-massively-expand-mail-in-voting_3676994.html", save = TRUE, path = "data1.html") 

get_features(article1)

article_html <- read_html(article1)

# get title
title <- article_html %>% 
  xml_find_all("//title") %>% 
  html_text()

# get autho's name <p><em> --> Get node <em> then extract the text
autho_name <- html_text(xml_find_all(article_html, "//em"))

# get post content class using . <div class="post_content">
post_content <- article_html %>% 
  html_nodes(css = ".post_content") %>% 
  html_text

# get date publish & update <span class="publish"> 
publish_date <- article_html %>% 
  html_nodes(css = ".publish") %>% 
  html_text()

publish_date_update <- article_html %>% 
  html_nodes(css = ".update") %>% 
  html_text()

# get description <meta name="description"
# select all meta nodes in the tree, then go to their children that have name = description. 
description <- article_html %>% 
  html_nodes(xpath = '//*[@id="main"]/div/div/div[1]/div[1]/div[2]/div[5]/p[1]') %>% 
  html_text()

tbl <- cbind(
       title, 
       autho_name, 
       publish_date,
       publish_date_update,
       description, post_content)

class(tbl)
```

# Function to get all 27 links from one page

```{r get_article_links}
# need to get all the links of post_list in a page, then get html of each link 
# <li class="post_list">  --> <a title= "mcbvksd" href="a link inside"
get_article_links <- function(url) {
  
one_page <- get_article(url = url,
                        save = TRUE, 
                        path = "first_page.html")
# convert to html file to extract elements in it. 
one_page <- read_html(one_page)

# get all the articles' links in this page
# go to all element that have class = "post_list", then go to the "a" descendant in the first position. ---> get 27 X 3 = 81 lines, 3 for each article 
raw_list <- one_page %>% 
  html_nodes(xpath = "//*[@class = 'post_list']//a[position() = 1]") 

# extract the link, starting the second in the list, increment by 3, 
urls <- raw_list[seq(2, length(raw_list), 3)] # 27 links (articles) in a page

# convert from html to character string in order to use stringr
urls_char <- as.character(urls)

# split the string to take only the link part
urls_char <- str_split(
  urls_char,
  pattern = ">",
  simplify = TRUE # to get a matrix (instead of a list), each column is an element being splited, here 27 x 3, the first column is the link we want.
)

# get rid of all the first part <a href=\" before the link
urls_char <- str_replace(urls_char[ ,1], # only get the first column which contains links
           pattern = '<a href=\"', 
           replacement = "")

# get rid of all the \" part at the end of the link
urls_clean <- str_replace(urls_char,
           pattern = '\"', 
           replacement = "")

return(urls_clean)
}

links_in_one_page <- get_article_links(url = "https://www.theepochtimes.com/c-us-politics/10")
```

# A for loop to get get all 27 articles from one page

```{r for loop getting all articles in 1 page}

for (url in 1:length(links_in_one_page)) {
# delay function get_article to run slowly
 get_article_delayed <- slowly(get_article, 
                             rate = rate_delay(1))
  
articles <- get_article_delayed(url = links_in_one_page[[url]], save = TRUE, path = "test.html")
  for (i in articles) {
    art_features  <- get_features(i)
  }
  
  print(art_features)
}

```

# Get all urls of politics section

```{r get_page_link, }
# A funtion to get page links
get_page_link <- function(page) {
  link <- "https://www.theepochtimes.com/c-us-politics"
  if(page > 1) {
    link <- paste0(link, "/", page)
  }
    link
}
# Get some page links, map through a vector of all pages 2:691
all_page_links <- map_chr(2:691, get_page_link) 
length(all_page_links)

# get all article links from above page links, 
all_article_links <- map(all_page_links, slowly(get_article_links,
                                                     rate = rate_delay(0.2))  
                         ) %>% 
                    unlist() 

length(all_article_links) # hooray 18657 articles to read

for (url in 1:length(all_article_links)) {
# delay function get_article to run slowly 1s
 get_article_delayed <- slowly(get_article, 
                             rate = rate_delay(1))
  
  articles <- get_article_delayed(url = all_article_links[[url]], save = TRUE, path = "test.html")
    
  for (i in articles) {
    # delay function get_features to run slowly 1s
    get_features_delayed <- slowly(get_features, 
                                   rate = rate_delay(1))
    
    art_features  <- get_features_delayed(i)
  }
  
  print(art_features)
}

```

# Side bar comment 

```{r}
# how to get the side bar comments <div id="sidebar_comments"></div>	
article_html %>% 
  html_nodes(css = "#sidebar_comments") %>% 
  html_text

article_html %>% 
html_nodes(xpath = "/html/body/main") %>% 
  html_text()

```


# Analysis

```{r analysis}
tbl %>% 
  unnest_tokens(word, content) %>% 
  anti_join(stop_words) %>% 
  count(word, sort = TRUE)
  View()
  
```

# Other

```{r unresolved}
# get keywords <meta name="keywords" using a xpath--> how to get up to "keywords"???
keyword <- xml_find_all(article_html, xpath = "//meta/@name/@content")
html_nodes(css = "#content")

```
